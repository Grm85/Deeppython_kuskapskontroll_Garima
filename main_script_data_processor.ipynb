{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87c79d47-65cc-4320-b17f-b0ad1822346e",
   "metadata": {},
   "source": [
    "### Dependendies\n",
    "\n",
    "##### pyodbc: Provides an interface to connect Python with ODBC databases,such as Microsoft SQL Server, allowing execution of SQL queries.\n",
    "##### sqlalchemy: Offers a SQL toolkit and Object-Relational Mapping (ORM) capabilities, simplifying database operations and query execution via Python.\n",
    "##### urllib: A standard Python library used for working with URLs, such as opening URLs and handling web-related requests and responses.\n",
    "##### logging: Part of Python's standard library, it allows tracking events during the execution of the program, useful for debugging and recording program flow or errors.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43937751-cf9f-4473-96e9-d14f19fc48a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import logging\n",
    "import pyodbc\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1ef3b6-bed8-4587-a62c-26fc880eedeb",
   "metadata": {},
   "source": [
    "### Setting up logging\n",
    "##### In this project, logging is used to track events, errors, and the overall flow of the application. The Python `logging` module is configured to write log messages to a file. This helps with debugging and auditing the execution of the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5857d54-c786-45bb-8455-3f8948b56c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(filename='data_processing.log', level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Function to log errors\n",
    "def log_error(error_message):\n",
    "    logging.error(f\"Error: {error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358dbb02-c111-4e13-a617-a9d507ac69ef",
   "metadata": {},
   "source": [
    "### Step 1: Load the CSV\n",
    "\n",
    "##### The first step in the data processing pipeline is loading the CSV file. This project uses the `pandas` library, which provides an efficient and easy way to load and manipulate CSV data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "435ac360-6a2c-47d1-b38b-13b43470836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='unicode_escape')\n",
    "        logging.info(\"CSV file loaded successfully.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        log_error(f\"Failed to load CSV: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99960c47-0a6d-471c-baef-0f9f6afa635a",
   "metadata": {},
   "source": [
    "### Step 2: Establish a connection to SQL Server\n",
    "\n",
    "##### After loading the CSV file, the next step is to establish a connection to the SQL Server. This project uses the `pyodbc` and `sqlalchemy` libraries to connect to a Microsoft SQL Server database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42274470-820c-4208-962d-3c9a0622ddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_sql_connection(server, database):\n",
    "    try:\n",
    "        # Using SQLAlchemy to create an engine\n",
    "        engine = create_engine(f\"mssql+pyodbc://{username}:{password}@{server}/{database}?driver=ODBC+Driver+17+for+SQL+Server\")\n",
    "        conn = pyodbc.connect(\n",
    "            trusted_connected='yes',\n",
    "            DRIVER='SQL Server',\n",
    "            SERVER=server,\n",
    "            DATABASE=database   \n",
    "        )\n",
    "        logging.info(\"Connected to SQL Server successfully.\")\n",
    "        return conn, engine\n",
    "    except Exception as e:\n",
    "        log_error(f\"Failed to connect to SQL Server: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989a7c9a-5336-441a-b1a8-ab811108afbe",
   "metadata": {},
   "source": [
    "### Step 3: Fetch data from the SQL table\n",
    "\n",
    "##### Once a connection to the SQL Server is established, the next step is to fetch data from the database. This project uses the `pandas` library in combination with the SQLAlchemy engine to retrieve data from a specific SQL table and load it into a DataFrame for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a829320-de9b-42eb-82e3-fd399d7708f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_data_from_sql(conn, table_name):\n",
    "    try:\n",
    "        query = f\"SELECT * FROM {table_name}\"\n",
    "        df_sql = pd.read_sql(query, conn)\n",
    "        logging.info(f\"Data fetched from {table_name} successfully.\")\n",
    "        return df_sql\n",
    "    except Exception as e:\n",
    "        log_error(f\"Failed to fetch data from SQL: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f9cea0-35a6-42cb-a0fa-1f3b45deac43",
   "metadata": {},
   "source": [
    "### Step 4: Data cleaning\n",
    "\n",
    "##### After fetching the data from the SQL table, itâ€™s important to clean the data to ensure it is suitable for analysis or further processing. Data cleaning involves handling missing values, correcting data types, removing duplicates, and addressing any inconsistencies in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "146e79b7-de51-4717-9587-cba2b19f5538",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_data(df_sql):\n",
    "    try:\n",
    "        # Dropping the 'Unnamed' and 'Status' columns\n",
    "        df_sql.drop(columns=['Unnamed', 'Status'], inplace=True, errors='ignore')\n",
    "\n",
    "        # Renaming 'nage' to 'age_group'\n",
    "        df_sql.rename(columns={'nage': 'age_group'}, inplace=True)\n",
    "\n",
    "        # Dropping duplicates\n",
    "        df_sql.drop_duplicates(inplace=True)\n",
    "\n",
    "        # Resetting the index\n",
    "        df_sql.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        logging.info(\"Data cleaning and transformation completed successfully.\")\n",
    "        return df_sql\n",
    "    except Exception as e:\n",
    "        log_error(f\"Data cleaning failed: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87edaf30-d287-443b-bc5b-19ca25f363ff",
   "metadata": {},
   "source": [
    "### Step 5: Save the updated data back to the SQL Server\n",
    "\n",
    "##### After cleaning and processing the data, the final step is to save the updated data back into the SQL Server database. This project uses the `to_sql()` function from the `pandas` library in combination with SQLAlchemy to write the DataFrame back to the SQL table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1af25b87-bb31-4a73-a5aa-948541a5f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_data_to_sql(df, engine, table_name):\n",
    "    try:\n",
    "        df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "        logging.info(f\"Data saved back to {table_name} successfully.\")\n",
    "    except Exception as e:\n",
    "        log_error(f\"Failed to save data to SQL Server: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dbea37-b069-4ed0-98a9-100782173fac",
   "metadata": {},
   "source": [
    "### Main function to execute the workflow\n",
    "\n",
    "##### The main function serves as the entry point for executing the entire data processing workflow. It orchestrates the sequence of operations, including loading the CSV, establishing a connection to SQL Server, fetching data, cleaning the data, and saving the updated data back to the SQL Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b1f07a0-e741-43f7-abee-aff613307b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20240\\4278941122.py:4: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_sql = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    csv_file_path = r\"C:\\Users\\LENOVO\\OneDrive\\Desktop\\deeppython\\Python_Diwali_Sales_Analysis-main\\data processor\\Diwali.csv\"\n",
    "    server = 'LAPTOP-U3795DN8'\n",
    "    database = 'mydatabase'\n",
    "    table_name = 'dbo.Diwali'\n",
    "\n",
    "    # Load CSV data\n",
    "    df = load_csv(csv_file_path)\n",
    "\n",
    "    # Connect to SQL Server\n",
    "    conn, engine = create_sql_connection(server, database,)\n",
    "\n",
    "    # Fetch data from SQL\n",
    "    df_sql = fetch_data_from_sql(conn, table_name)\n",
    "\n",
    "    # Clean data\n",
    "    cleaned_df = clean_data(df_sql)\n",
    "\n",
    "    # Save the cleaned data back to SQL\n",
    "    save_data_to_sql(cleaned_df, engine, table_name)\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "    logging.info(\"Database connection closed.\")\n",
    "\n",
    "def create_sql_connection(server, database):\n",
    "    conn_str = (\n",
    "        f\"Driver={{ODBC Driver 17 for SQL Server}};\"\n",
    "        f\"Server={server};\"\n",
    "        f\"Database={database};\"\n",
    "        \"Trusted_Connection=yes;\"\n",
    "    )\n",
    "    engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={urllib.parse.quote_plus(conn_str)}\")\n",
    "    conn = pyodbc.connect(conn_str)\n",
    "    return conn, engine\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b70d038-7835-44ec-af59-f5f245d312b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59e4152-d654-4b47-884b-97ae3cbad47a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
